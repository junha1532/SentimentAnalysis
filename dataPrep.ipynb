{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "######Stanford Sentiment Treebank######################\n",
    "sstb = pd.read_csv(\"datasetSentences.txt\", sep = \"\\t\", usecols=[1])\n",
    "datasetsplit = pd.read_csv(\"datasetSplit.txt\", sep = \",\", usecols=[1])\n",
    "\n",
    "sstb = pd.concat([sstb,datasetsplit], axis=1)\n",
    "\n",
    "sstb_train = sstb[sstb[\"splitset_label\"] == 1]\n",
    "sstb_test = sstb[sstb[\"splitset_label\"] == 2]\n",
    "sstb_validation = sstb[sstb[\"splitset_label\"] == 3]\n",
    "##########Stanford Twitter Sentiment Corpus#############\n",
    "\n",
    "skip = sorted(rd.sample(range(1600000), 1600000-80000))\n",
    "sts_train = pd.read_csv(\"training.csv\", encoding = \"ISO-8859-1\", engine='python', sep=\",\", header=None, skiprows = skip)\n",
    "sts_test = pd.read_csv(\"testdata.csv\", sep=\",\", header = None)\n",
    "skip = sorted(rd.sample(range(1600000), 1600000-16000))\n",
    "sts_validation = pd.read_csv(\"training.csv\", encoding = \"ISO-8859-1\", engine=\"python\", sep=\",\", header=None, skiprows = skip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467837384</td>\n",
       "      <td>Mon Apr 06 22:26:42 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kenandise</td>\n",
       "      <td>Behind on my classes for work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467844140</td>\n",
       "      <td>Mon Apr 06 22:28:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Twokids1</td>\n",
       "      <td>@rumblepurr lol.. wish they understood dayligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467889231</td>\n",
       "      <td>Mon Apr 06 22:40:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>luannem</td>\n",
       "      <td>@kariajay  All this time you didn't notice I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467900244</td>\n",
       "      <td>Mon Apr 06 22:43:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Mowgli3</td>\n",
       "      <td>http://twitpic.com/2y2yi - I love you, Buck.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467916510</td>\n",
       "      <td>Mon Apr 06 22:47:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Daniiej</td>\n",
       "      <td>omg i've an economics test. and i dont know al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15995</td>\n",
       "      <td>4</td>\n",
       "      <td>2193346429</td>\n",
       "      <td>Tue Jun 16 08:20:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChanelBlueSatin</td>\n",
       "      <td>@Neevy89 lol well people say you can get the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15996</td>\n",
       "      <td>4</td>\n",
       "      <td>2193474371</td>\n",
       "      <td>Tue Jun 16 08:30:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>H_Anderson3</td>\n",
       "      <td>@sundayrain haha. Thanks.. Yeah I got that tat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15997</td>\n",
       "      <td>4</td>\n",
       "      <td>2193476073</td>\n",
       "      <td>Tue Jun 16 08:30:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xLukesterx</td>\n",
       "      <td>@NicciLuvsMollie no problem ;) and awww  I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15998</td>\n",
       "      <td>4</td>\n",
       "      <td>2193576427</td>\n",
       "      <td>Tue Jun 16 08:38:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>wonder_nat</td>\n",
       "      <td>@AndrewDearling *yawns*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15999</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0           1                             2         3                4  \\\n",
       "0      0  1467837384  Mon Apr 06 22:26:42 PDT 2009  NO_QUERY        kenandise   \n",
       "1      0  1467844140  Mon Apr 06 22:28:32 PDT 2009  NO_QUERY         Twokids1   \n",
       "2      0  1467889231  Mon Apr 06 22:40:24 PDT 2009  NO_QUERY          luannem   \n",
       "3      0  1467900244  Mon Apr 06 22:43:26 PDT 2009  NO_QUERY          Mowgli3   \n",
       "4      0  1467916510  Mon Apr 06 22:47:56 PDT 2009  NO_QUERY          Daniiej   \n",
       "...   ..         ...                           ...       ...              ...   \n",
       "15995  4  2193346429  Tue Jun 16 08:20:01 PDT 2009  NO_QUERY  ChanelBlueSatin   \n",
       "15996  4  2193474371  Tue Jun 16 08:30:27 PDT 2009  NO_QUERY      H_Anderson3   \n",
       "15997  4  2193476073  Tue Jun 16 08:30:35 PDT 2009  NO_QUERY       xLukesterx   \n",
       "15998  4  2193576427  Tue Jun 16 08:38:45 PDT 2009  NO_QUERY       wonder_nat   \n",
       "15999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   RyanTrevMorris   \n",
       "\n",
       "                                                       5  \n",
       "0                         Behind on my classes for work   \n",
       "1      @rumblepurr lol.. wish they understood dayligh...  \n",
       "2      @kariajay  All this time you didn't notice I w...  \n",
       "3          http://twitpic.com/2y2yi - I love you, Buck.   \n",
       "4      omg i've an economics test. and i dont know al...  \n",
       "...                                                  ...  \n",
       "15995  @Neevy89 lol well people say you can get the s...  \n",
       "15996  @sundayrain haha. Thanks.. Yeah I got that tat...  \n",
       "15997  @NicciLuvsMollie no problem ;) and awww  I'm s...  \n",
       "15998                           @AndrewDearling *yawns*   \n",
       "15999  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[16000 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467816149</td>\n",
       "      <td>Mon Apr 06 22:21:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Pbearfox</td>\n",
       "      <td>@julieebaby awe i love you too!!!! 1 am here  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467820906</td>\n",
       "      <td>Mon Apr 06 22:22:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>voyage2k</td>\n",
       "      <td>@localtweeps Wow, tons of replies from you, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467825084</td>\n",
       "      <td>Mon Apr 06 22:23:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PresidentSnow</td>\n",
       "      <td>@Lt_Algonquin agreed, I saw the failwhale alll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467833799</td>\n",
       "      <td>Mon Apr 06 22:25:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kaelaaa</td>\n",
       "      <td>i think my arms are sore from tennis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3              4  \\\n",
       "0  0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY        mybirch   \n",
       "1  0  1467816149  Mon Apr 06 22:21:11 PDT 2009  NO_QUERY       Pbearfox   \n",
       "2  0  1467820906  Mon Apr 06 22:22:24 PDT 2009  NO_QUERY       voyage2k   \n",
       "3  0  1467825084  Mon Apr 06 22:23:30 PDT 2009  NO_QUERY  PresidentSnow   \n",
       "4  0  1467833799  Mon Apr 06 22:25:46 PDT 2009  NO_QUERY        kaelaaa   \n",
       "\n",
       "                                                   5  \n",
       "0                                        Need a hug   \n",
       "1  @julieebaby awe i love you too!!!! 1 am here  ...  \n",
       "2  @localtweeps Wow, tons of replies from you, ma...  \n",
       "3  @Lt_Algonquin agreed, I saw the failwhale alll...  \n",
       "4              i think my arms are sore from tennis   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train.shape #80,000 rows and 6 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.0\n",
       "4    0.0\n",
       "3    0.0\n",
       "2    0.0\n",
       "1    0.0\n",
       "0    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train.isnull().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isPlayer Has Died! Sorry                                                                                                               10\n",
       "Goodnight                                                                                                                               7\n",
       "headache                                                                                                                                7\n",
       "Headache                                                                                                                                6\n",
       "I am so so so so so bored of studying                                                                                                   5\n",
       "                                                                                                                                       ..\n",
       "800 words, and screen shots in under 30min, that has to be a new record for me. Moving Image assignment done, just to finish radio      1\n",
       "@ChrisHardie  you might want to look up the definition for vegan again                                                                  1\n",
       "Looking for remote flash triggers.  Can't find what I want.     Back shelf that idea for now.                                           1\n",
       "got my hair cut!  It's like short :O                                                                                                    1\n",
       "I'm wondering why I always have to stay in on the days when it's unbelievably sunny.                                                    1\n",
       "Name: 5, Length: 79796, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train[5].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          Need a hug \n",
       "1    @julieebaby awe i love you too!!!! 1 am here  ...\n",
       "2    @localtweeps Wow, tons of replies from you, ma...\n",
       "3    @Lt_Algonquin agreed, I saw the failwhale alll...\n",
       "4                i think my arms are sore from tennis \n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train[5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sts_train = sts_train.drop([0,1,3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467816149</td>\n",
       "      <td>Mon Apr 06 22:21:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Pbearfox</td>\n",
       "      <td>@julieebaby awe i love you too!!!! 1 am here  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467820906</td>\n",
       "      <td>Mon Apr 06 22:22:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>voyage2k</td>\n",
       "      <td>@localtweeps Wow, tons of replies from you, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467825084</td>\n",
       "      <td>Mon Apr 06 22:23:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PresidentSnow</td>\n",
       "      <td>@Lt_Algonquin agreed, I saw the failwhale alll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467833799</td>\n",
       "      <td>Mon Apr 06 22:25:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kaelaaa</td>\n",
       "      <td>i think my arms are sore from tennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79995</td>\n",
       "      <td>4</td>\n",
       "      <td>2193576360</td>\n",
       "      <td>Tue Jun 16 08:38:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MandyStocker</td>\n",
       "      <td>dropping molly off getting ice cream with Aaro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79996</td>\n",
       "      <td>4</td>\n",
       "      <td>2193576815</td>\n",
       "      <td>Tue Jun 16 08:38:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sarasmile13</td>\n",
       "      <td>@gone2dmb agreed!  Soulful eyes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79997</td>\n",
       "      <td>4</td>\n",
       "      <td>2193577517</td>\n",
       "      <td>Tue Jun 16 08:38:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stephyway</td>\n",
       "      <td>oooo haha just waking up and ready to eat a de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79998</td>\n",
       "      <td>4</td>\n",
       "      <td>2193577671</td>\n",
       "      <td>Tue Jun 16 08:38:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>craiglparker</td>\n",
       "      <td>@siovene lol I don't blame you it's not the sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79999</td>\n",
       "      <td>4</td>\n",
       "      <td>2193578679</td>\n",
       "      <td>Tue Jun 16 08:38:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>puchal_ek</td>\n",
       "      <td>@myheartandmind jo jen by nemuselo zrovna tÃƒÂ© ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0           1                             2         3              4  \\\n",
       "0      0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY        mybirch   \n",
       "1      0  1467816149  Mon Apr 06 22:21:11 PDT 2009  NO_QUERY       Pbearfox   \n",
       "2      0  1467820906  Mon Apr 06 22:22:24 PDT 2009  NO_QUERY       voyage2k   \n",
       "3      0  1467825084  Mon Apr 06 22:23:30 PDT 2009  NO_QUERY  PresidentSnow   \n",
       "4      0  1467833799  Mon Apr 06 22:25:46 PDT 2009  NO_QUERY        kaelaaa   \n",
       "...   ..         ...                           ...       ...            ...   \n",
       "79995  4  2193576360  Tue Jun 16 08:38:44 PDT 2009  NO_QUERY   MandyStocker   \n",
       "79996  4  2193576815  Tue Jun 16 08:38:47 PDT 2009  NO_QUERY    sarasmile13   \n",
       "79997  4  2193577517  Tue Jun 16 08:38:51 PDT 2009  NO_QUERY      stephyway   \n",
       "79998  4  2193577671  Tue Jun 16 08:38:51 PDT 2009  NO_QUERY   craiglparker   \n",
       "79999  4  2193578679  Tue Jun 16 08:38:56 PDT 2009  NO_QUERY      puchal_ek   \n",
       "\n",
       "                                                       5  \n",
       "0                                            Need a hug   \n",
       "1      @julieebaby awe i love you too!!!! 1 am here  ...  \n",
       "2      @localtweeps Wow, tons of replies from you, ma...  \n",
       "3      @Lt_Algonquin agreed, I saw the failwhale alll...  \n",
       "4                  i think my arms are sore from tennis   \n",
       "...                                                  ...  \n",
       "79995  dropping molly off getting ice cream with Aaro...  \n",
       "79996                 @gone2dmb agreed!  Soulful eyes.    \n",
       "79997  oooo haha just waking up and ready to eat a de...  \n",
       "79998  @siovene lol I don't blame you it's not the sa...  \n",
       "79999  @myheartandmind jo jen by nemuselo zrovna tÃƒÂ© ...  \n",
       "\n",
       "[80000 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sts_train_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polarity: 0 = negative, 2 = neutral, 4 = positive\n",
    "sts_train.rename(columns={0:'polarity', 1: 'tweet_id', 2:'date', 4:'username', 5:'tweet_text'}, inplace=True)\n",
    "sts_validation.rename(columns={0:'polarity', 1: 'tweet_id', 2:'date', 4:'username', 5:'tweet_text'}, inplace=True)\n",
    "sts_test.rename(columns={0:'polarity', 1: 'tweet_id', 2:'date', 4:'username', 5:'tweet_text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_train = sts_train.drop([3], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467816149</td>\n",
       "      <td>Mon Apr 06 22:21:11 PDT 2009</td>\n",
       "      <td>Pbearfox</td>\n",
       "      <td>@julieebaby awe i love you too!!!! 1 am here  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467820906</td>\n",
       "      <td>Mon Apr 06 22:22:24 PDT 2009</td>\n",
       "      <td>voyage2k</td>\n",
       "      <td>@localtweeps Wow, tons of replies from you, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467825084</td>\n",
       "      <td>Mon Apr 06 22:23:30 PDT 2009</td>\n",
       "      <td>PresidentSnow</td>\n",
       "      <td>@Lt_Algonquin agreed, I saw the failwhale alll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467833799</td>\n",
       "      <td>Mon Apr 06 22:25:46 PDT 2009</td>\n",
       "      <td>kaelaaa</td>\n",
       "      <td>i think my arms are sore from tennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79995</td>\n",
       "      <td>4</td>\n",
       "      <td>2193576360</td>\n",
       "      <td>Tue Jun 16 08:38:44 PDT 2009</td>\n",
       "      <td>MandyStocker</td>\n",
       "      <td>dropping molly off getting ice cream with Aaro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79996</td>\n",
       "      <td>4</td>\n",
       "      <td>2193576815</td>\n",
       "      <td>Tue Jun 16 08:38:47 PDT 2009</td>\n",
       "      <td>sarasmile13</td>\n",
       "      <td>@gone2dmb agreed!  Soulful eyes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79997</td>\n",
       "      <td>4</td>\n",
       "      <td>2193577517</td>\n",
       "      <td>Tue Jun 16 08:38:51 PDT 2009</td>\n",
       "      <td>stephyway</td>\n",
       "      <td>oooo haha just waking up and ready to eat a de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79998</td>\n",
       "      <td>4</td>\n",
       "      <td>2193577671</td>\n",
       "      <td>Tue Jun 16 08:38:51 PDT 2009</td>\n",
       "      <td>craiglparker</td>\n",
       "      <td>@siovene lol I don't blame you it's not the sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79999</td>\n",
       "      <td>4</td>\n",
       "      <td>2193578679</td>\n",
       "      <td>Tue Jun 16 08:38:56 PDT 2009</td>\n",
       "      <td>puchal_ek</td>\n",
       "      <td>@myheartandmind jo jen by nemuselo zrovna tÃƒÂ© ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       polarity    tweet_id                          date       username  \\\n",
       "0             0  1467811592  Mon Apr 06 22:20:03 PDT 2009        mybirch   \n",
       "1             0  1467816149  Mon Apr 06 22:21:11 PDT 2009       Pbearfox   \n",
       "2             0  1467820906  Mon Apr 06 22:22:24 PDT 2009       voyage2k   \n",
       "3             0  1467825084  Mon Apr 06 22:23:30 PDT 2009  PresidentSnow   \n",
       "4             0  1467833799  Mon Apr 06 22:25:46 PDT 2009        kaelaaa   \n",
       "...         ...         ...                           ...            ...   \n",
       "79995         4  2193576360  Tue Jun 16 08:38:44 PDT 2009   MandyStocker   \n",
       "79996         4  2193576815  Tue Jun 16 08:38:47 PDT 2009    sarasmile13   \n",
       "79997         4  2193577517  Tue Jun 16 08:38:51 PDT 2009      stephyway   \n",
       "79998         4  2193577671  Tue Jun 16 08:38:51 PDT 2009   craiglparker   \n",
       "79999         4  2193578679  Tue Jun 16 08:38:56 PDT 2009      puchal_ek   \n",
       "\n",
       "                                              tweet_text  \n",
       "0                                            Need a hug   \n",
       "1      @julieebaby awe i love you too!!!! 1 am here  ...  \n",
       "2      @localtweeps Wow, tons of replies from you, ma...  \n",
       "3      @Lt_Algonquin agreed, I saw the failwhale alll...  \n",
       "4                  i think my arms are sore from tennis   \n",
       "...                                                  ...  \n",
       "79995  dropping molly off getting ice cream with Aaro...  \n",
       "79996                 @gone2dmb agreed!  Soulful eyes.    \n",
       "79997  oooo haha just waking up and ready to eat a de...  \n",
       "79998  @siovene lol I don't blame you it's not the sa...  \n",
       "79999  @myheartandmind jo jen by nemuselo zrovna tÃƒÂ© ...  \n",
       "\n",
       "[80000 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sts_train.tweet_text\n",
    "y = sts_train.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# SEED = 2000\n",
    "# x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "# x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19596    wonders why the change of status approval noti...\n",
       "30172    New computer get here NOW. I don't want to wai...\n",
       "49792    Belanja Kaos itu gampang! www.kaoskita.com T-s...\n",
       "27870    just saw an ad for rush *tear* I cried so much...\n",
       "58902    Congratulations @garyvee, Lizzie and family &a...\n",
       "                               ...                        \n",
       "79483                 @LVMAKEUP we had a great weekend... \n",
       "34358       @this_is_tors just a cheeky 17. My knee hurts \n",
       "69581    just finished miXINgTpes   ha!! time to sleep ...\n",
       "62670          Watching my favorite people play softball. \n",
       "37704    Ughhh. Twitter is not playing nice again this ...\n",
       "Name: tweet_text, Length: 78400, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "79995    4\n",
       "79996    4\n",
       "79997    4\n",
       "79998    4\n",
       "79999    4\n",
       "Name: polarity, Length: 80000, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19596    0\n",
       "30172    0\n",
       "49792    4\n",
       "27870    0\n",
       "58902    4\n",
       "        ..\n",
       "79483    4\n",
       "34358    0\n",
       "69581    4\n",
       "62670    4\n",
       "37704    0\n",
       "Name: polarity, Length: 78400, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(\"Train set has total {0} entries with {1:.2f}% negative, {1:.2f}% positive, {1:.2f}% neutral\".format(len(x),\n",
    "#                                                                     (len(x[y == 0]) / (len(x)*1.))*100,\n",
    "#                                                                     (len(x[y == 4]) / (len(x)*1.))*100,\n",
    "#                                                                     (len(x[y == 2]) / (len(x)*1.))*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           Behind on my classes for work \n",
       "1        @rumblepurr lol.. wish they understood dayligh...\n",
       "2        @kariajay  All this time you didn't notice I w...\n",
       "3            http://twitpic.com/2y2yi - I love you, Buck. \n",
       "4        omg i've an economics test. and i dont know al...\n",
       "                               ...                        \n",
       "15995    @Neevy89 lol well people say you can get the s...\n",
       "15996    @sundayrain haha. Thanks.. Yeah I got that tat...\n",
       "15997    @NicciLuvsMollie no problem ;) and awww  I'm s...\n",
       "15998                             @AndrewDearling *yawns* \n",
       "15999    happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "Name: tweet_text, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_validation = sts_validation.tweet_text\n",
    "x_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "15995    4\n",
       "15996    4\n",
       "15997    4\n",
       "15998    4\n",
       "15999    4\n",
       "Name: polarity, Length: 16000, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation = sts_validation.polarity\n",
    "y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "1      Reading my kindle2...  Love it... Lee childs i...\n",
       "2      Ok, first assesment of the #kindle2 ...it fuck...\n",
       "3      @kenburbary You'll love your Kindle2. I've had...\n",
       "4      @mikefish  Fair enough. But i have the Kindle2...\n",
       "                             ...                        \n",
       "493    Ask Programming: LaTeX or InDesign?: submitted...\n",
       "494    On that note, I hate Word. I hate Pages. I hat...\n",
       "495    Ahhh... back in a *real* text editing environm...\n",
       "496    Trouble in Iran, I see. Hmm. Iran. Iran so far...\n",
       "497    Reading the tweets coming out of Iran... The w...\n",
       "Name: tweet_text, Length: 498, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = sts_test.tweet_text\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      4\n",
       "1      4\n",
       "2      4\n",
       "3      4\n",
       "4      4\n",
       "      ..\n",
       "493    2\n",
       "494    0\n",
       "495    4\n",
       "496    0\n",
       "497    0\n",
       "Name: polarity, Length: 498, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = sts_test.polarity\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = pd.concat([x, x_validation, x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2154120.17it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2366718.21it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2121722.72it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2389817.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2548127.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 1680745.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2398527.65it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 1725706.17it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2318388.51it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2481815.69it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2000325.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2327052.69it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 1971552.32it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2528783.71it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2504808.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2505257.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2433484.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2253561.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2400932.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2432694.30it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2470574.99it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2366192.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 1739733.10it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2401929.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2325528.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2346629.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2429161.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2520202.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2504033.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2308510.70it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2483734.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 717 ms, total: 1min 17s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_cbow.alpha -= 0.002\n",
    "    model_ug_cbow.min_alpha = model_ug_cbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2252357.84it/s]\n"
     ]
    }
   ],
   "source": [
    "model_ug_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2354574.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2391752.63it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2477865.27it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2467171.47it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2334879.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2563637.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2574121.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2464137.32it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2315894.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2397419.49it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2320275.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2348849.19it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2371822.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2267500.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2310777.64it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2347132.92it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2191003.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2327387.22it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2297946.70it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2339575.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2211330.03it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2318534.60it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2201191.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2216306.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2327266.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2329973.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2336725.85it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2247118.23it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2049098.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96498/96498 [00:00<00:00, 2337360.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 26s, sys: 831 ms, total: 2min 27s\n",
      "Wall time: 44.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_sg.alpha -= 0.002\n",
    "    model_ug_sg.min_alpha = model_ug_sg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow.save('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg.save('w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_ug_cbow = KeyedVectors.load('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42019"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_ug_cbow.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42019 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84337"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need a hug \n",
      "@julieebaby awe i love you too!!!! 1 am here  i miss you\n",
      "@localtweeps Wow, tons of replies from you, may have to unfollow so I can see my friends' tweets, you're scrolling the feed a lot. \n",
      "@Lt_Algonquin agreed, I saw the failwhale allllll day today. \n",
      "i think my arms are sore from tennis \n"
     ]
    }
   ],
   "source": [
    "for words in x[:5]:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[90, 4, 1071],\n",
       " [23976, 1723, 1, 44, 7, 45, 195, 63, 91, 1, 87, 7],\n",
       " [23977,\n",
       "  258,\n",
       "  2057,\n",
       "  12,\n",
       "  2341,\n",
       "  54,\n",
       "  7,\n",
       "  331,\n",
       "  16,\n",
       "  2,\n",
       "  5105,\n",
       "  15,\n",
       "  1,\n",
       "  66,\n",
       "  69,\n",
       "  5,\n",
       "  9974,\n",
       "  445,\n",
       "  157,\n",
       "  15364,\n",
       "  3,\n",
       "  1621,\n",
       "  4,\n",
       "  299],\n",
       " [156, 23978, 2058, 1, 309, 3, 15365, 15366, 30, 40],\n",
       " [1, 78, 5, 1979, 36, 619, 54, 1946]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same data as sequential data \n",
    "sequences[:5] #Each word is represented as a number \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "x2 = x\n",
    "for words in x:\n",
    "    length.append(len(words.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (80000, 50)\n"
     ]
    }
   ],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=50)\n",
    "print('Shape of data tensor:', x_train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,    90,     4,  1071],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0, 23976,  1723,     1,    44,     7,    45,   195,\n",
       "           63,    91,     1,    87,     7],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0, 23977,\n",
       "          258,  2057,    12,  2341,    54,     7,   331,    16,     2,\n",
       "         5105,    15,     1,    66,    69,     5,  9974,   445,   157,\n",
       "        15364,     3,  1621,     4,   299],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,   156, 23978,  2058,     1,   309,\n",
       "            3, 15365, 15366,    30,    40],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     1,    78,     5,\n",
       "         1979,    36,   619,    54,  1946]], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_seq[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(embedding_matrix[1071] ,embeddings_index.get('hug'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 200)           20000000  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 49, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,100\n",
      "Trainable params: 20,040,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "structure_test = Sequential()\n",
    "e = Embedding(100000, 200, input_length=50)\n",
    "structure_test.add(e)\n",
    "structure_test.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "structure_test.add(GlobalMaxPooling1D())\n",
    "\n",
    "structure_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:414: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 16000 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-12dd32f73ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_cnn_02\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_cnn_02\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_cnn_02\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_cnn_02 = Sequential()\n",
    "e = Embedding(100000, 200, input_length=50)\n",
    "model_cnn_02.add(e)\n",
    "model_cnn_02.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn_02.add(GlobalMaxPooling1D())\n",
    "model_cnn_02.add(Dense(256, activation='relu'))\n",
    "model_cnn_02.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn_02.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_02.fit(x_train_seq, y, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
